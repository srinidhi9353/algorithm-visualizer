export function generateAIResponse(userMessage: string): string {
  const lowerMessage = userMessage.toLowerCase();
  
  // Merge sort
  if (lowerMessage.includes('merge sort') || lowerMessage.includes('mergesort')) {
    return `ğŸ”€ **Merge Sort - The Reliable Divide & Conquer Algorithm**

**ğŸ”§ How It Works:**
1. **Divide**: Split array into two halves recursively
2. **Conquer**: Sort each half recursively
3. **Merge**: Combine two sorted halves into one sorted array
4. **Base Case**: Single element (already sorted)

**ğŸ“Š Performance:**
â€¢ **All Cases**: O(n log n) - Guaranteed!
â€¢ **Space**: O(n) - Requires auxiliary array
â€¢ **Stable**: Yes - maintains relative order of equal elements

**ğŸ† Key Advantages:**
âœ… **Predictable**: Always O(n log n), no worst case surprises
âœ… **Stable Sort**: Preserves order of equal elements
âœ… **Parallelizable**: Easy to implement in parallel
âœ… **External Sorting**: Great for sorting data that doesn't fit in memory

**âš ï¸ Disadvantages:**
âŒ Extra space needed (O(n))
âŒ Not in-place sorting
âŒ Slower than Quick Sort on average for arrays

**ğŸ’¡ When to Use:**
- Need guaranteed O(n log n) performance
- Stability is important
- Sorting linked lists (no extra space needed)
- External sorting of large datasets
- Parallel processing environments

**ğŸ¯ Real-World Usage:**
- Java's Collections.sort() for objects
- Python's sorted() and list.sort()
- Sorting large files that don't fit in RAM
- Database systems

ğŸ® See the merge process visually in our Merge Sort visualizer!`;
  }
  
  // Heap sort
  if (lowerMessage.includes('heap sort') || lowerMessage.includes('heapsort')) {
    return `ğŸ”ï¸ **Heap Sort - In-Place with Guaranteed Performance**

**ğŸ”§ How It Works:**
1. **Build Max Heap**: Transform array into max heap structure
2. **Extract Max**: Swap root (max) with last element
3. **Heapify**: Restore heap property for remaining elements
4. **Repeat**: Until all elements are sorted

**ğŸ“Š Performance:**
â€¢ **All Cases**: O(n log n) - Guaranteed!
â€¢ **Space**: O(1) - In-place sorting
â€¢ **Stable**: No - doesn't preserve equal element order

**ğŸ† Advantages:**
âœ… **Guaranteed O(n log n)**: No worst-case degradation
âœ… **In-Place**: No extra memory needed
âœ… **No Recursion Overhead**: Iterative implementation possible
âœ… **Good for Priority Queues**: Natural fit

**âš ï¸ Disadvantages:**
âŒ Not stable
âŒ Slower than Quick Sort on average
âŒ Poor cache locality

**ğŸ’¡ When to Use:**
- Need guaranteed O(n log n) without extra space
- Memory is constrained
- Don't need stability
- Implementing priority queues

**ğŸ¯ Heap Property:**
- **Max Heap**: Parent â‰¥ Children
- **Min Heap**: Parent â‰¤ Children

ğŸ® Watch heap building and sorting in our visualizer!`;
  }
  
  // Bubble sort
  if (lowerMessage.includes('bubble sort') || lowerMessage.includes('bubblesort')) {
    return `ğŸ«§ **Bubble Sort - The Teaching Algorithm**

**ğŸ”§ How It Works:**
1. Compare adjacent elements
2. Swap if they're in wrong order
3. Repeat until no swaps needed
4. Largest element "bubbles up" each pass

**ğŸ“Š Performance:**
â€¢ **Best Case**: O(n) - Already sorted (with optimization)
â€¢ **Average**: O(nÂ²) - Random order
â€¢ **Worst Case**: O(nÂ²) - Reverse sorted
â€¢ **Space**: O(1) - In-place
â€¢ **Stable**: Yes

**âœ… Advantages:**
- Simple to understand and implement
- Detects already sorted arrays efficiently (optimized version)
- Stable sort
- In-place

**âŒ Disadvantages:**
- Very slow for large datasets
- O(nÂ²) makes it impractical for real use
- Many unnecessary comparisons

**ğŸ’¡ When to Use:**
- Teaching purposes ONLY
- Tiny datasets (< 10 elements)
- Nearly sorted data with optimized version
- Educational demonstrations

**ğŸ¯ Optimization:**
Add flag to detect if any swaps occurred. If no swaps in a pass, array is sorted!

**Real Talk:** Almost never used in production. Learn it to understand sorting concepts, then use better algorithms!

ğŸ® Perfect for visualizing how sorting works!`;
  }
  
  // Insertion sort  
  if (lowerMessage.includes('insertion sort') || lowerMessage.includes('insertionsort')) {
    return `ğŸ“Œ **Insertion Sort - Simple & Efficient for Small Data**

**ğŸ”§ How It Works:**
1. Start with first element (considered sorted)
2. Take next element
3. Insert it into correct position in sorted portion
4. Shift larger elements right
5. Repeat for all elements

**ğŸ“Š Performance:**
â€¢ **Best Case**: O(n) - Already sorted
â€¢ **Average**: O(nÂ²) - Random order  
â€¢ **Worst Case**: O(nÂ²) - Reverse sorted
â€¢ **Space**: O(1) - In-place
â€¢ **Stable**: Yes

**ğŸ† Advantages:**
âœ… **Adaptive**: O(n) for nearly sorted data
âœ… **Online**: Can sort data as it arrives
âœ… **Stable**: Preserves equal element order
âœ… **In-Place**: No extra memory
âœ… **Simple**: Easy to implement
âœ… **Low Overhead**: Fast for small arrays

**ğŸ’¡ When to Use:**
- Small datasets (< 50 elements)
- Nearly sorted data
- Online sorting (streaming data)
- As part of hybrid algorithms (like Timsort)
- When simplicity matters

**ğŸ¯ Real-World Usage:**
- Quicksort switches to insertion sort for small subarrays
- Timsort (Python's default) uses it for small runs
- Shell sort is advanced version of insertion sort

**Fun Fact:** Like sorting playing cards in your hand!

ğŸ® See the insertion process step-by-step!`;
  }
  
  // Selection sort
  if (lowerMessage.includes('selection sort') || lowerMessage.includes('selectionsort')) {
    return `ğŸ¯ **Selection Sort - Find Minimum, Repeat**

**ğŸ”§ How It Works:**
1. Find minimum element in unsorted portion
2. Swap it with first unsorted element
3. Move boundary of sorted portion forward
4. Repeat until array is sorted

**ğŸ“Š Performance:**
â€¢ **All Cases**: O(nÂ²) - Always the same!
â€¢ **Space**: O(1) - In-place
â€¢ **Stable**: No (can be made stable with modifications)
â€¢ **Swaps**: O(n) - Minimal swaps

**âœ… Advantages:**
- Minimal number of swaps: O(n)
- Good when write operations are expensive
- Simple implementation
- In-place sorting

**âŒ Disadvantages:**
- Always O(nÂ²) - doesn't adapt to sorted data
- Not stable in standard form
- Slow for large datasets

**ğŸ’¡ When to Use:**
- Write operations are very expensive
- Small datasets
- Memory is extremely limited
- Teaching sorting concepts

**ğŸ¯ Special Use Case:**
Flash memory/EEPROM where writes are expensive but reads are cheap.

**Comparison with Bubble Sort:**
- Selection: Fewer swaps, always O(nÂ²)
- Bubble: More swaps, can be O(n) for sorted data

Both are mainly educational algorithms!

ğŸ® Watch minimum selection in action!`;
  }
  
  // Binary search
  if (lowerMessage.includes('binary search') && !lowerMessage.includes('tree')) {
    return `ğŸ¯ **Binary Search - The Logarithmic Power**

**ğŸ”§ How It Works:**
1. **Prerequisite**: Array MUST be sorted
2. **Compare**: Check middle element
3. **Decide**: Target < middle? Search left half
4. **Decide**: Target > middle? Search right half
5. **Found**: Target == middle? Done!
6. **Repeat**: Until found or no elements left

**ğŸ“Š Performance:**
â€¢ **All Cases**: O(log n) - Extremely fast!
â€¢ **Space**: O(1) iterative, O(log n) recursive
â€¢ **Requires**: Sorted array

**ğŸ† Why It's Amazing:**
âœ… **Speed**: Log n means 1 billion items â†’ 30 comparisons max!
âœ… **Efficient**: Eliminates half the search space each step
âœ… **Scalable**: Works great for large datasets

**ğŸ’¡ When to Use:**
- Searching in sorted array/list
- Finding insertion position
- Finding first/last occurrence
- Range queries in sorted data

**ğŸ¯ Variations:**
- **Lower Bound**: First element â‰¥ target
- **Upper Bound**: First element > target
- **Exact Match**: Find specific element
- **Rotated Array**: Modified binary search

**âš ï¸ Common Mistakes:**
1. Forgetting array must be sorted
2. Infinite loop: Use mid = left + (right - left) / 2
3. Integer overflow: Avoid (left + right) / 2
4. Off-by-one errors in boundaries

**ğŸ“š Real-World:**
- Dictionary lookup
- Database indexing  
- Version control (git bisect)
- Finding bugs in code history

**ğŸ® Interactive Demo:**
See how search space halves with each comparison!

**Pro Tip:** Master binary search - it's in 50%+ of coding interviews!`;
  }
  
  // Linear search
  if (lowerMessage.includes('linear search')) {
    return `ğŸ” **Linear Search - The Simple Sequential Approach**

**ğŸ”§ How It Works:**
1. Start at first element
2. Check if it matches target
3. If yes, return index
4. If no, move to next element
5. Repeat until found or end of array

**ğŸ“Š Performance:**
â€¢ **Best Case**: O(1) - Target is first element
â€¢ **Average**: O(n/2) = O(n) - Target in middle
â€¢ **Worst Case**: O(n) - Target at end or not present
â€¢ **Space**: O(1)

**âœ… Advantages:**
- Works on unsorted data
- Simple to implement
- No preprocessing needed
- Works on any data structure (arrays, linked lists)

**âŒ Disadvantages:**
- Slow for large datasets
- Doesn't take advantage of sorted data

**ğŸ’¡ When to Use:**
- Small datasets (< 100 elements)
- Unsorted data
- Searching linked lists
- One-time searches
- When simplicity is priority

**ğŸ¯ Comparison:**
| Feature | Linear | Binary |
|---------|--------|--------|
| Sorted? | No | Yes |
| Time | O(n) | O(log n) |
| Simple? | Very | Moderate |

**Real-World:**
- Finding item in shopping cart
- Checking if user exists in small list
- Validating input against small whitelist

**Optimization:** Use sentinels to eliminate boundary checks!

ğŸ® Watch sequential search in action!`;
  }
  
  // Hash table / HashMap
  if (lowerMessage.includes('hash') && (lowerMessage.includes('table') || lowerMessage.includes('map'))) {
    return `#ï¸âƒ£ **Hash Tables - O(1) Lookup Magic**

**ğŸ”§ How It Works:**
1. **Hash Function**: Converts key â†’ index
2. **Store**: Place value at computed index
3. **Retrieve**: Hash key again to find value
4. **Handle Collisions**: When two keys hash to same index

**ğŸ“Š Performance:**
â€¢ **Average**: O(1) - Insert, Delete, Search
â€¢ **Worst Case**: O(n) - All keys hash to same slot
â€¢ **Space**: O(n)

**ğŸ”‘ Collision Resolution:**

**1. Chaining:**
- Each slot contains linked list
- Multiple values can share same slot
- Simple and flexible

**2. Open Addressing:**
- Find next available slot
- Linear probing: Check next slot
- Quadratic probing: Check iÂ² slots away
- Double hashing: Use second hash function

**ğŸ† Advantages:**
âœ… **Fast Average**: O(1) for all operations
âœ… **Flexible Keys**: Any hashable type
âœ… **Constant Time**: Regardless of size

**âš ï¸ Disadvantages:**
âŒ No ordering maintained
âŒ Space overhead
âŒ Worst case can be O(n)
âŒ Hash function quality critical

**ğŸ’¡ Good Hash Function Properties:**
- Deterministic: Same input â†’ Same output
- Uniform distribution: Minimize collisions
- Fast to compute
- Minimize patterns

**ğŸ¯ When to Use:**
- Need fast lookup/insert/delete
- Key-value storage
- Caching
- Counting frequencies
- Detecting duplicates
- Implementing sets

**ğŸ“š Real-World:**
- Database indexing
- Caching (LRU cache)
- Symbol tables in compilers
- Browser history
- DNS resolution
- Password storage (with cryptographic hash)

**âš¡ Load Factor:**
Î» = n / m (items / slots)
- Keep Î» < 0.7 for good performance
- Rehash when load factor gets high

**Common Implementations:**
- JavaScript: Object, Map
- Python: dict
- Java: HashMap
- C++: unordered_map

ğŸ® Visualize hashing and collision resolution!`;
  }
  
  // Time Complexity
  if (lowerMessage.includes('time complexity') || lowerMessage.includes('big o')) {
    return `ğŸ•’ **Time Complexity Analysis**

Time complexity describes how runtime grows as input size increases!

**ğŸ“Š Common Complexities (Best to Worst):**

â€¢ **O(1)** - Constant: Array access, hash lookup
â€¢ **O(log n)** - Logarithmic: Binary search, balanced trees
â€¢ **O(n)** - Linear: Single loop, linear search
â€¢ **O(n log n)** - Linearithmic: Merge sort, heap sort
â€¢ **O(nÂ²)** - Quadratic: Nested loops, bubble sort
â€¢ **O(2â¿)** - Exponential: Recursive Fibonacci, subset generation

**ğŸ¯ Pro Tips:**
- Always consider worst-case scenarios
- Nested loops often indicate O(nÂ²)
- Divide-and-conquer usually gives O(n log n)
- DP can reduce exponential to polynomial

ğŸ” Want specifics? Ask about a particular algorithm!`;
  }
  
  // BFS vs DFS
  if (lowerMessage.includes('bfs') && lowerMessage.includes('dfs')) {
    return `ğŸŒ³ **BFS vs DFS Comparison**

**BFS (Breadth-First Search):**
ğŸ”„ Strategy: Explores level by level
ğŸ“¦ Uses: Queue (FIFO)
ğŸ¯ Best For: Shortest path in unweighted graphs
âš¡ Space: O(V) - can use more memory
âœ… Finds minimum distance

**DFS (Depth-First Search):**
ğŸ”„ Strategy: Goes as deep as possible first
ğŸ“¦ Uses: Stack (LIFO) or recursion
ğŸ¯ Best For: Exploring all paths, detecting cycles
âš¡ Space: O(h) - more memory efficient
âœ… Better for "existence" problems

**ğŸ® Real Applications:**
- BFS: Social media connections, GPS routes, web crawling
- DFS: Maze solving, topological sorting, cycle detection

ğŸš€ Try our graph visualizer to see both in action!`;
  }
  
  // Quick Sort
  if (lowerMessage.includes('quick sort') || lowerMessage.includes('quicksort')) {
    return `âš¡ **Quick Sort - The Speed Demon!**

**ğŸ”§ How It Works:**
1. Choose a pivot element
2. Partition: smaller left, larger right
3. Recursively sort left and right subarrays
4. No merge needed - sorts in place!

**ğŸ“Š Performance:**
â€¢ Best/Average: O(n log n)
â€¢ Worst: O(nÂ²) - poor pivot choice
â€¢ Space: O(log n) - recursion stack

**ğŸ† Why It's Amazing:**
âœ… In-place sorting
âœ… Cache-friendly
âœ… Used in most standard libraries
âœ… Parallelizable

**ğŸ’¡ Optimization Tricks:**
- Random pivot avoids worst-case
- 3-way partitioning handles duplicates
- Switch to insertion sort for small arrays

ğŸ® See it in action with our visualizer!`;
  }

  // Dynamic Programming
  if (lowerMessage.includes('dynamic programming') || lowerMessage.includes(' dp ') || lowerMessage.includes('when') && lowerMessage.includes('dp')) {
    return `ğŸ§  **Dynamic Programming Guide**

**ğŸ¯ When to Use DP:**
âœ… Optimal substructure - optimal solution contains optimal subsolutions
âœ… Overlapping subproblems - same problems solved multiple times
âœ… Optimization problems - finding max/min or counting

**ğŸ”„ Two Approaches:**

**Memoization (Top-Down):**
- Start with original problem
- Break into subproblems recursively
- Cache results
- More intuitive

**Tabulation (Bottom-Up):**
- Start with smallest subproblems
- Build up to original
- Fill table systematically
- More space-efficient

**ğŸ† Classic Problems:**
- Fibonacci: O(2â¿) â†’ O(n)
- 0/1 Knapsack: Max value within weight
- LCS: Longest Common Subsequence
- Edit Distance: String transformation
- Coin Change: Minimum coins needed

**ğŸ’¡ Problem-Solving Steps:**
1. Identify optimal substructure
2. Define recurrence relation
3. Choose memoization or tabulation
4. Implement solution
5. Optimize space if possible

ğŸš€ Try our DP visualizer to see subproblems build up!`;
  }

  // Machine Learning
  if (lowerMessage.includes('machine learning') || lowerMessage.includes(' ml ') || lowerMessage.includes(' ai ')) {
    return `ğŸ¤– **Machine Learning Basics**

**ğŸ¯ Three Main Types:**

**1. Supervised Learning** ğŸ“š
- Learn from labeled examples
- Examples: Classification, regression
- Algorithms: Linear regression, decision trees, neural networks

**2. Unsupervised Learning** ğŸ”
- Find patterns in unlabeled data
- Examples: Clustering, dimensionality reduction
- Algorithms: K-means, PCA, autoencoders

**3. Reinforcement Learning** ğŸ®
- Learn through trial and error with rewards
- Examples: Game playing, robotics
- Algorithms: Q-learning, policy gradients

**ğŸ”§ Essential Algorithms:**
- Linear Regression: Predict continuous values
- Decision Trees: Easy to interpret
- Neural Networks: Complex pattern recognition
- K-Means: Group similar data points

**ğŸ¯ ML Workflow:**
1. Data Collection
2. Preprocessing
3. Model Selection
4. Training
5. Evaluation
6. Deployment
7. Monitoring

**ğŸ’¡ Key Concepts:**
- Overfitting vs Underfitting
- Cross-Validation
- Feature Engineering
- Bias-Variance Tradeoff

ğŸ® Explore our ML visualizers!`;
  }

  // Recursion
  if (lowerMessage.includes('recursion') || lowerMessage.includes('recursive')) {
    return `ğŸ”„ **Recursion Explained**

A function calling itself with smaller inputs!

**ğŸ—ï¸ Anatomy:**
1. **Base Case** ğŸ›‘ - Stopping condition
2. **Recursive Case** ğŸ”„ - Function calls itself with smaller input

**ğŸ“š Classic Examples:**
- Factorial: n Ã— factorial(n-1)
- Fibonacci: fib(n-1) + fib(n-2)
- Tree Traversal: Natural recursive structure

**ğŸ’¡ When to Use:**
âœ… Tree/Graph problems
âœ… Divide & Conquer
âœ… Mathematical sequences
âœ… Backtracking
âœ… Parsing nested structures

**âš ï¸ Pitfalls:**
âŒ Stack overflow - too many calls
âŒ Inefficiency - repeated calculations
âŒ Memory usage - stack space per call

**ğŸš€ Optimizations:**
- Memoization: Cache results
- Tail Recursion: Last operation is recursive call
- Convert to Iteration: Use explicit stack

**ğŸ§  Thinking Recursively:**
1. Identify the pattern
2. Find the base case
3. Define recursive relation
4. Trust the recursion!

ğŸ® Watch recursion visualizers to see call stack!`;
  }

  // Sorting algorithms
  if (lowerMessage.includes('sorting') || lowerMessage.includes('sort algorithm')) {
    return `ğŸ”„ **Sorting Algorithms Overview**

**ğŸ¯ Main Categories:**

**Simple Sorts (O(nÂ²)):**
- **Bubble Sort**: Swap adjacent, largest bubbles up
- **Selection Sort**: Find minimum, place at front
- **Insertion Sort**: Insert each into sorted portion
- âœ… Good for: Small arrays, nearly sorted data

**Efficient Sorts (O(n log n)):**
- **Merge Sort**: Divide, sort, merge - stable & predictable
- **Quick Sort**: Pivot partition - fast average case
- **Heap Sort**: Binary heap - in-place & O(n log n) guarantee
- âœ… Good for: Large datasets, general purpose

**Specialized Sorts:**
- **Counting Sort**: O(n+k) for integers in range
- **Radix Sort**: O(dÃ—n) for fixed-length numbers
- **Bucket Sort**: O(n) average for uniform distribution

**ğŸ“Š Comparison:**
| Algorithm | Time | Space | Stable |
|-----------|------|-------|--------|
| Bubble    | O(nÂ²)| O(1)  | Yes    |
| Merge     | O(n log n)| O(n) | Yes |
| Quick     | O(n log n)| O(log n) | No |
| Heap      | O(n log n)| O(1) | No |

**ğŸ’¡ Choosing the Right One:**
- **Need stability?** â†’ Merge Sort
- **Memory constrained?** â†’ Heap Sort
- **General purpose?** â†’ Quick Sort
- **Nearly sorted?** â†’ Insertion Sort
- **Small array?** â†’ Insertion Sort

ğŸ® Use our visualizers to compare them side-by-side!`;
  }

  // Searching algorithms
  if (lowerMessage.includes('search') && (lowerMessage.includes('algorithm') || lowerMessage.includes('binary') || lowerMessage.includes('linear'))) {
    return `ğŸ” **Searching Algorithms**

**ğŸ¯ Common Search Methods:**

**Linear Search:**
- Check each element sequentially
- Time: O(n)
- Space: O(1)
- âœ… Works on unsorted data
- âœ… Simple to implement
- Use when: Small dataset or unsorted

**Binary Search:**
- Divide and conquer on sorted data
- Time: O(log n)
- Space: O(1) iterative, O(log n) recursive
- âœ… Very fast for large datasets
- âŒ Requires sorted array
- Use when: Large sorted dataset

**Jump Search:**
- Jump ahead by âˆšn steps
- Time: O(âˆšn)
- Between linear and binary
- Use when: Binary search is complex to implement

**Interpolation Search:**
- Smart guessing based on value
- Time: O(log log n) for uniform data
- Use when: Data uniformly distributed

**Hash-Based Search:**
- Direct access via hash function
- Time: O(1) average
- Space: O(n) for hash table
- Use when: Need fastest possible lookup

**ğŸŒ³ Tree-Based Search:**
- Binary Search Trees: O(log n) balanced
- B-Trees: Efficient for databases
- Tries: Great for string prefix matching

**ğŸ’¡ Selection Guide:**
- **Unsorted data?** â†’ Linear Search
- **Sorted array?** â†’ Binary Search
- **Need O(1)?** â†’ Hash Table
- **Prefix matching?** â†’ Trie
- **Range queries?** â†’ Binary Search Tree

ğŸ® Try our search visualizers!`;
  }

  // Graph algorithms
  if (lowerMessage.includes('graph') || lowerMessage.includes('dijkstra') || lowerMessage.includes('shortest path')) {
    return `ğŸŒ **Graph Algorithms**

**ğŸ¯ Essential Graph Algorithms:**

**Traversal:**
- **BFS**: Level-order, shortest path (unweighted)
- **DFS**: Explore deep, cycle detection

**Shortest Path:**
- **Dijkstra's**: Weighted graphs, non-negative weights
- **Bellman-Ford**: Handles negative weights
- **A***: Heuristic-guided, optimal & efficient
- **Floyd-Warshall**: All pairs shortest path

**Minimum Spanning Tree:**
- **Prim's**: Grow tree from single vertex
- **Kruskal's**: Sort edges, add if no cycle

**Advanced:**
- **Topological Sort**: Order with dependencies
- **Strongly Connected Components**: Tarjan's/Kosaraju's
- **Network Flow**: Max flow problems

**ğŸ“Š Complexity Comparison:**
| Algorithm | Time | Use Case |
|-----------|------|----------|
| BFS | O(V+E) | Unweighted shortest path |
| DFS | O(V+E) | Cycle detection, paths |
| Dijkstra | O((V+E)log V) | Weighted shortest path |
| Bellman-Ford | O(VE) | Negative weights |
| A* | O(E) | Optimal heuristic search |

**ğŸ¯ When to Use:**
- **Unweighted shortest path?** â†’ BFS
- **Weighted shortest path?** â†’ Dijkstra's or A*
- **Negative weights?** â†’ Bellman-Ford
- **Detect cycles?** â†’ DFS
- **All pairs?** â†’ Floyd-Warshall

ğŸ—ºï¸ Explore with our graph visualizer!`;
  }

  // Arrays and strings
  if (lowerMessage.includes('array') || lowerMessage.includes('string')) {
    return `ğŸ“Š **Arrays & Strings**

**ğŸ¯ Common Operations:**

**Array Basics:**
- Access: O(1)
- Search: O(n) unsorted, O(log n) sorted
- Insert: O(n) middle, O(1) end
- Delete: O(n) middle, O(1) end

**Classic Array Problems:**
- Two Sum / Two Pointers
- Sliding Window technique
- Kadane's Algorithm (max subarray)
- Dutch National Flag (3-way partition)
- Rotate array
- Find duplicates

**String Operations:**
- Palindrome checking
- Anagram detection
- String matching (KMP, Rabin-Karp)
- Longest substring problems
- String reversal

**ğŸ’¡ Common Patterns:**

**Two Pointers:**
- One slow, one fast
- Both ends moving inward
- Use for: Pairs, palindromes, partitioning

**Sliding Window:**
- Maintain window of elements
- Expand/contract as needed
- Use for: Subarray problems, longest/shortest substring

**Hash Map:**
- Track frequencies, indices
- O(1) lookup
- Use for: Duplicates, pairs summing to target

**Prefix Sum:**
- Precompute cumulative sums
- O(1) range queries
- Use for: Subarray sum problems

**ğŸš€ Optimization Tips:**
- Use sets for O(1) lookups
- Sort first if order doesn't matter
- Consider space-time tradeoffs
- In-place modifications when possible

ğŸ¯ Practice these patterns on our platform!`;
  }

  // Data structures
  if (lowerMessage.includes('data structure') || lowerMessage.includes('stack') || lowerMessage.includes('queue') || lowerMessage.includes('tree') || lowerMessage.includes('heap')) {
    return `ğŸ“¦ **Data Structures Guide**

**ğŸ¯ Fundamental Structures:**

**Linear Structures:**
- **Array**: O(1) access, fixed size
- **Linked List**: O(1) insert/delete at head, dynamic
- **Stack** (LIFO): Push/pop O(1), undo mechanisms
- **Queue** (FIFO): Enqueue/dequeue O(1), task scheduling

**Hierarchical:**
- **Binary Tree**: Hierarchical data, O(log n) operations (balanced)
- **BST**: Sorted tree, efficient search/insert
- **Heap**: Priority queue, O(log n) insert/extract
- **Trie**: Prefix tree, string operations

**Hash-Based:**
- **Hash Table**: O(1) average lookup/insert
- **Hash Set**: Unique elements, O(1) contains

**Advanced:**
- **Graph**: Vertices & edges, networks/relationships
- **Disjoint Set**: Union-find, connected components
- **Segment Tree**: Range queries, O(log n) updates

**ğŸ“Š Selection Guide:**

**Need LIFO?** â†’ Stack
**Need FIFO?** â†’ Queue  
**Fast lookup?** â†’ Hash Table
**Sorted data?** â†’ BST or Heap
**Priority handling?** â†’ Heap
**Hierarchical?** â†’ Tree
**Networks?** â†’ Graph
**Prefix matching?** â†’ Trie

**ğŸ’¡ Trade-offs:**
- Arrays: Fast access, fixed size
- Linked Lists: Dynamic, slower access
- Trees: Balanced operations, complex
- Hash Tables: Fast average, no order

ğŸ® Build and explore structures interactively!`;
  }

  // Complexity analysis
  if (lowerMessage.includes('complexity') || lowerMessage.includes('analyze')) {
    return `ğŸ“ˆ **Complexity Analysis**

**â±ï¸ Time Complexity:**
Measures how runtime grows with input size

**Common Classes:**
- O(1): Constant - hash lookup
- O(log n): Logarithmic - binary search
- O(n): Linear - loop through all
- O(n log n): Efficient sort - merge/quick sort
- O(nÂ²): Quadratic - nested loops
- O(2â¿): Exponential - recursive Fibonacci

**ğŸ’¾ Space Complexity:**
Measures memory usage growth

**Common Cases:**
- O(1): Constant - few variables
- O(log n): Recursion depth - binary search
- O(n): Linear - extra array
- O(nÂ²): Matrix - 2D array

**ğŸ” How to Analyze:**

**1. Identify Operations:**
- Count significant operations
- Ignore constants

**2. Look for Loops:**
- Single loop: O(n)
- Nested loops: Multiply complexities
- Sequential loops: Add complexities

**3. Recursion:**
- Draw recursion tree
- Count nodes and work per node
- T(n) = branches Ã— T(n/divisor) + work

**4. Amortized Analysis:**
- Average over sequence of operations
- Dynamic array resize: O(1) amortized

**ğŸ’¡ Master Theorem:**
For T(n) = aT(n/b) + O(n^d):
- If a > b^d: O(n^log_b(a))
- If a = b^d: O(n^d Ã— log n)
- If a < b^d: O(n^d)

**ğŸ¯ Practical Tips:**
- Focus on worst-case usually
- Constants matter in practice
- Consider both time AND space
- Optimization: Lower complexity first, then constants

ğŸ“Š Practice analyzing with our algorithm visualizers!`;
  }

  // Binary Search Tree
  if (lowerMessage.includes('bst') || lowerMessage.includes('binary search tree')) {
    return `ğŸŒ³ **Binary Search Tree (BST) - Ordered Tree Structure**

**ğŸ”§ BST Property:**
For every node:
- **Left subtree**: All values < node value
- **Right subtree**: All values > node value
- This property holds recursively

**ğŸ“Š Performance:**
â€¢ **Balanced Tree**: O(log n) - Search, Insert, Delete
â€¢ **Unbalanced Tree**: O(n) - Worst case (becomes linked list)
â€¢ **Space**: O(n)

**ğŸ” Operations:**

**Search**: Compare, go left if smaller, right if larger - O(log n) balanced
**Insert**: Find position, add as leaf - O(log n) balanced
**Delete**: 3 cases - leaf, one child, two children - O(log n) balanced

**ğŸ¯ Traversals:**
- **Inorder**: Gives sorted order!
- **Preorder**: Root first, good for copying tree
- **Postorder**: Root last, good for deleting tree
- **Level-order**: BFS, level by level

**ğŸ’¡ When to Use:**
Dynamic sorted data, range queries, finding predecessor/successor

ğŸ® Build and traverse BSTs interactively!`;
  }
  
  // Stack
  if (lowerMessage.includes('stack') && !lowerMessage.includes('call stack')) {
    return `ğŸª¬ **Stack - LIFO (Last In, First Out)**

**ğŸ”§ Core Operations:**
- **Push**: Add to top - O(1)
- **Pop**: Remove from top - O(1)
- **Peek**: View top - O(1)
- **IsEmpty**: Check if empty - O(1)

**ğŸ¯ Real-World Uses:**
1. **Function Call Stack**: Stores function calls and local variables
2. **Undo/Redo**: Text editors, Photoshop
3. **Expression Evaluation**: Postfix, infix to postfix
4. **Backtracking**: DFS, maze solving, puzzle solving
5. **Syntax Parsing**: Balanced parentheses, compilers

**ğŸ“ Classic Problems:**
- Balanced parentheses: Push '(', pop and match ')'
- Reverse string: Push all, pop all
- Next greater element: Stack-based O(n) solution

**ğŸ’¡ When to Use:**
LIFO order, recursion to iteration, backtracking, expression parsing

ğŸ® Watch stack operations visually!`;
  }
  
  // Queue  
  if (lowerMessage.includes('queue') && !lowerMessage.includes('priority')) {
    return `ğŸš¦ **Queue - FIFO (First In, First Out)**

**ğŸ”§ Core Operations:**
- **Enqueue**: Add to rear - O(1)
- **Dequeue**: Remove from front - O(1)
- **Front/Peek**: View front - O(1)
- **IsEmpty**: Check if empty - O(1)

**ğŸ¯ Real-World Uses:**
1. **Task Scheduling**: CPU, print queue, thread pools
2. **BFS Traversal**: Level-order, shortest path
3. **Buffering**: IO buffers, streaming, message queues
4. **Async Processing**: Event handling, callbacks
5. **Resource Sharing**: Printer queue, downloads

**ğŸŒŸ Variants:**
- **Deque**: Insert/remove both ends
- **Circular Queue**: Efficient space usage
- **Priority Queue**: Serve by priority (heap)

**ğŸ“ Classic Problems:**
- Generate binary numbers 1 to n
- Sliding window maximum (deque)
- Level order tree traversal

**ğŸ’¡ When to Use:**
FIFO order, BFS, scheduling, buffering, producer-consumer

ğŸ® See queue operations in action!`;
  }
  
  // Linked List
  if (lowerMessage.includes('linked list')) {
    return `ğŸ”— **Linked List - Dynamic Linear Structure**

**ğŸ“š Types:**
1. **Singly**: One pointer (next) - forward only
2. **Doubly**: Two pointers (next, prev) - both directions
3. **Circular**: Last points to first - no null end

**ğŸ“Š Operations:**
- **Insert at head**: O(1)
- **Insert at tail**: O(1) with tail pointer, O(n) without
- **Delete**: O(1) if node given, O(n) if searching
- **Search**: O(n) - must traverse

**âœ… Advantages:**
Dynamic size, efficient insert/delete at known position, no wasted space

**âŒ Disadvantages:**
No random access, extra memory for pointers, cache unfriendly

**ğŸ“ Classic Problems:**
- **Reverse list**: 3 pointers or recursive
- **Detect cycle**: Floyd's algorithm (tortoise & hare)
- **Find middle**: Slow/fast pointers
- **Merge sorted lists**: Compare and link
- **Remove Nth from end**: Two pointers n apart

**ğŸ¯ Array vs List:**
- Array: O(1) access, O(n) insert/delete, contiguous
- List: O(n) access, O(1) insert/delete*, scattered
*At known position

**ğŸ’¡ When to Use:**
Frequent insertions/deletions, unknown size, implement stacks/queues

ğŸ® Visualize node connections!`;
  }
  
  // Priority Queue / Heap
  if (lowerMessage.includes('priority queue') || (lowerMessage.includes('heap') && !lowerMessage.includes('sort'))) {
    return `ğŸ† **Priority Queue - Serve by Priority**

**ğŸ“š Concept:**
Elements have priority - highest priority served first!
Usually implemented with **Binary Heap**.

**ğŸ”§ Heap Operations:**
- **Insert**: Add element, bubble up - O(log n)
- **Extract-Max/Min**: Remove root, bubble down - O(log n)
- **Peek**: View root - O(1)
- **Build Heap**: From array - O(n)

**ğŸ¯ Heap Property:**
- **Max Heap**: Parent â‰¥ all children
- **Min Heap**: Parent â‰¤ all children

**ğŸ“Š Performance:**
All operations O(log n) except peek O(1) and build O(n)

**ğŸ’¡ When to Use:**
- Find kth largest/smallest element
- Merge k sorted arrays
- Task scheduling by priority
- Dijkstra's shortest path
- Huffman coding
- Median of stream

**ğŸ“ Classic Problems:**
- **Top K elements**: Use min heap of size k
- **Median of stream**: Two heaps (max for lower, min for upper)
- **Merge k sorted lists**: Min heap with heads

**ğŸ¯ Heap vs BST:**
- Heap: Fast min/max, not fully sorted
- BST: Fully sorted, slower min/max

**Real Applications:**
- OS task scheduling
- Event simulation
- Network packet routing
- Load balancing

ğŸ® Watch heap operations live!`;
  }
  
  // Trie
  if (lowerMessage.includes('trie') || lowerMessage.includes('prefix tree')) {
    return `ğŸŒ³ **Trie - Prefix Tree for Strings**

**ğŸ“š Concept:**
Tree where each path represents a string.
Each node has children for each character.

**ğŸ”§ Operations:**
- **Insert**: Add word character by character - O(m) where m = word length
- **Search**: Follow path for word - O(m)
- **StartsWith**: Check if prefix exists - O(m)
- **Delete**: Remove word, clean up unused nodes - O(m)

**ğŸ† Advantages:**
âœ… **Prefix matching**: Very fast
âœ… **Autocomplete**: Natural fit
âœ… **Spell checking**: Find similar words
âœ… **No collisions**: Unlike hash tables

**âŒ Disadvantages:**
âŒ Space intensive: O(ALPHABET_SIZE Ã— N Ã— M)
âŒ More complex than hash table

**ğŸ’¡ When to Use:**
- Autocomplete features
- Spell checkers
- IP routing (longest prefix match)
- Dictionary implementations
- Word games (Boggle solver)

**ğŸ¯ Real Applications:**
- Search engine autocomplete
- T9 predictive text
- Browser URL suggestions
- Contact name search
- Network routing tables

**ğŸ“ Trie vs Hash Table:**
| Feature | Trie | Hash |
|---------|------|------|
| Prefix search | O(m) | O(nÃ—m) |
| Space | More | Less |
| Collisions | No | Yes |
| Ordered | Yes | No |

**ğŸ”¥ Compressed Trie:**
Radix tree - compress chains of single-child nodes to save space!

ğŸ® Build tries and search prefixes!`;
  }
  
  // Graph representations
  if (lowerMessage.includes('graph') && (lowerMessage.includes('represent') || lowerMessage.includes('adjacency'))) {
    return `ğŸ—ºï¸ **Graph Representations**

**ğŸ¯ Two Main Ways:**

**1. Adjacency Matrix:**
A 2D array where matrix[i][j] = 1 if edge exists from i to j, else 0

**Performance:**
- Space: O(VÂ²)
- Check edge: O(1)
- Find neighbors: O(V)
- Add edge: O(1)
- Add vertex: O(VÂ²) - resize matrix

**Best for:**
âœ… Dense graphs (many edges)
âœ… Frequent edge lookups
âœ… Small graphs

**2. Adjacency List:**
Array of lists where list[i] contains all neighbors of vertex i

**Performance:**
- Space: O(V + E)
- Check edge: O(degree)
- Find neighbors: O(degree)
- Add edge: O(1)
- Add vertex: O(1)

**Best for:**
âœ… Sparse graphs (few edges)
âœ… Most real-world graphs
âœ… Large graphs
âœ… Dynamic graphs

**ğŸ“Š Comparison:**

| Operation | Matrix | List |
|-----------|--------|------|
| Space | O(VÂ²) | O(V+E) |
| Add edge | O(1) | O(1) |
| Check edge | O(1) | O(V) |
| Neighbors | O(V) | O(1) |

**ğŸ’¡ When to Use:**

**Matrix if:**
- Dense graph (E close to VÂ²)
- Frequent edge existence checks
- Small graph
- Matrix operations needed

**List if:**
- Sparse graph (E much less than VÂ²)
- Iterate over neighbors often
- Large graph
- Dynamic (add/remove edges)

**ğŸŒŸ For Weighted Graphs:**
- Matrix: store weight instead of 1
- List: store (neighbor, weight) pairs

**Real Usage:**
Most algorithms use adjacency lists - real graphs are usually sparse!

ğŸ® See both representations side by side!`;
  }

  // Default intelligent responses
  const intelligentResponses = [
    `ğŸ¤” **Interesting Question!**

I'd love to help! Could you provide more details?

**For better assistance, tell me:**
- What specific algorithm or concept?
- Are you looking for explanation, implementation, or comparison?
- Any particular use case or problem you're solving?
- What programming language do you prefer?

**ğŸ’¡ I can help with:**
- Algorithm explanations (BFS, DFS, Quick Sort, etc.)
- Data structure operations
- Complexity analysis
- Problem-solving strategies
- Code debugging
- Interview preparation

**ğŸ¯ Example questions:**
- "Explain how binary search works"
- "When should I use a heap vs BST?"
- "Compare merge sort and quick sort"
- "Help me optimize this O(nÂ²) solution"

What would you like to learn about?`,

    `ğŸ¯ **Great Topic!**

I can provide detailed explanations! To give you the best answer:

**ğŸ“š Choose what you need:**
- **Concept Explanation**: How does it work?
- **Implementation**: Show me the code
- **Comparison**: vs other approaches
- **Use Cases**: When to use it?
- **Complexity**: Time and space analysis

**ğŸ”§ Available Topics:**
- Sorting (Quick, Merge, Heap, Bubble, etc.)
- Searching (Binary, Linear, etc.)
- Graphs (BFS, DFS, Dijkstra, A*)
- Trees (BST, AVL, Heap, Trie)
- Dynamic Programming
- Data Structures (Stack, Queue, etc.)
- Machine Learning basics

**ğŸ’¡ Pro Tip:** Be specific! Instead of "tell me about sorting", try "explain why quick sort is O(nÂ²) worst case"

What specific aspect interests you?`,

    `ğŸš€ **Excellent Question!**

**ğŸ“ Let me guide your learning:**

**For Concepts:**
- Ask: "Explain [algorithm/concept]"
- Example: "Explain breadth-first search"

**For Comparisons:**
- Ask: "Compare X vs Y"
- Example: "Compare stack vs queue"

**For Problems:**
- Describe your scenario
- Example: "Find duplicates in array efficiently"

**For Code:**
- Ask for implementation
- Specify language preference

**ğŸ”¥ Popular Topics:**
- Time complexity & Big O
- Graph algorithms
- Sorting techniques
- Dynamic programming
- Data structure selection

**ğŸ® Interactive Learning:**
- Use our visualizers
- Try algorithm games
- Take level assessments
- Track code execution

**ğŸ’¡ Current Page:** You're on the ${lowerMessage.includes('current') ? 'chat page' : 'platform'}. Click "Explain Current Page" for context-specific help!

What would you like to explore?`,
  ];

  const randomResponse = intelligentResponses[Math.floor(Math.random() * intelligentResponses.length)];
  return randomResponse;
}
